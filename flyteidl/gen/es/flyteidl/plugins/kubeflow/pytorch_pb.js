// @generated by protoc-gen-es v1.4.2 with parameter "target=js+dts+ts,keep_empty_files=false"
// @generated from file flyteidl/plugins/kubeflow/pytorch.proto (package flyteidl.plugins.kubeflow, syntax proto3)
/* eslint-disable */
// @ts-nocheck

import { proto3 } from "@bufbuild/protobuf";
import { RestartPolicy, RunPolicy } from "./common_pb.js";
import { Resources } from "../../core/tasks_pb.js";

/**
 * Custom proto for torch elastic config for distributed training using 
 * https://github.com/kubeflow/training-operator/blob/master/pkg/apis/kubeflow.org/v1/pytorch_types.go
 *
 * @generated from message flyteidl.plugins.kubeflow.ElasticConfig
 */
export const ElasticConfig = proto3.makeMessageType(
  "flyteidl.plugins.kubeflow.ElasticConfig",
  () => [
    { no: 1, name: "rdzv_backend", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 2, name: "min_replicas", kind: "scalar", T: 5 /* ScalarType.INT32 */ },
    { no: 3, name: "max_replicas", kind: "scalar", T: 5 /* ScalarType.INT32 */ },
    { no: 4, name: "nproc_per_node", kind: "scalar", T: 5 /* ScalarType.INT32 */ },
    { no: 5, name: "max_restarts", kind: "scalar", T: 5 /* ScalarType.INT32 */ },
  ],
);

/**
 * Proto for plugin that enables distributed training using https://github.com/kubeflow/pytorch-operator
 *
 * @generated from message flyteidl.plugins.kubeflow.DistributedPyTorchTrainingTask
 */
export const DistributedPyTorchTrainingTask = proto3.makeMessageType(
  "flyteidl.plugins.kubeflow.DistributedPyTorchTrainingTask",
  () => [
    { no: 1, name: "worker_replicas", kind: "message", T: DistributedPyTorchTrainingReplicaSpec },
    { no: 2, name: "master_replicas", kind: "message", T: DistributedPyTorchTrainingReplicaSpec },
    { no: 3, name: "run_policy", kind: "message", T: RunPolicy },
    { no: 4, name: "elastic_config", kind: "message", T: ElasticConfig },
  ],
);

/**
 * @generated from message flyteidl.plugins.kubeflow.DistributedPyTorchTrainingReplicaSpec
 */
export const DistributedPyTorchTrainingReplicaSpec = proto3.makeMessageType(
  "flyteidl.plugins.kubeflow.DistributedPyTorchTrainingReplicaSpec",
  () => [
    { no: 1, name: "replicas", kind: "scalar", T: 5 /* ScalarType.INT32 */ },
    { no: 2, name: "image", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 3, name: "resources", kind: "message", T: Resources },
    { no: 4, name: "restart_policy", kind: "enum", T: proto3.getEnumType(RestartPolicy) },
  ],
);

