// @generated by protoc-gen-es v1.4.2 with parameter "target=js+dts+ts,keep_empty_files=false"
// @generated from file flyteidl/plugins/pytorch.proto (package flyteidl.plugins, syntax proto3)
/* eslint-disable */
// @ts-nocheck

import { proto3 } from "@bufbuild/protobuf";

/**
 * Custom proto for torch elastic config for distributed training using 
 * https://github.com/kubeflow/training-operator/blob/master/pkg/apis/kubeflow.org/v1/pytorch_types.go
 *
 * @generated from message flyteidl.plugins.ElasticConfig
 */
export const ElasticConfig = proto3.makeMessageType(
  "flyteidl.plugins.ElasticConfig",
  () => [
    { no: 1, name: "rdzv_backend", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 2, name: "min_replicas", kind: "scalar", T: 5 /* ScalarType.INT32 */ },
    { no: 3, name: "max_replicas", kind: "scalar", T: 5 /* ScalarType.INT32 */ },
    { no: 4, name: "nproc_per_node", kind: "scalar", T: 5 /* ScalarType.INT32 */ },
    { no: 5, name: "max_restarts", kind: "scalar", T: 5 /* ScalarType.INT32 */ },
  ],
);

/**
 * Custom proto for plugin that enables distributed training using https://github.com/kubeflow/pytorch-operator
 *
 * @generated from message flyteidl.plugins.DistributedPyTorchTrainingTask
 */
export const DistributedPyTorchTrainingTask = proto3.makeMessageType(
  "flyteidl.plugins.DistributedPyTorchTrainingTask",
  () => [
    { no: 1, name: "workers", kind: "scalar", T: 5 /* ScalarType.INT32 */ },
    { no: 2, name: "elastic_config", kind: "message", T: ElasticConfig },
  ],
);

